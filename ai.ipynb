{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee20c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noahbradwin/Coswara-Data/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "class OptimizedAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, tensor_dir, strict_join=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the metadata CSV (e.g., 'combined_data.csv').\n",
    "            tensor_dir (str): Path to folder containing pre-processed .pt files.\n",
    "            strict_join (bool): If True, filters out rows where the .pt file is missing.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.tensor_dir = tensor_dir\n",
    "        \n",
    "        # 1. Strict Join: Filter out rows where the pre-processed tensor doesn't exist\n",
    "        if strict_join:\n",
    "            self.df['file_exists'] = self.df['id'].apply(\n",
    "                lambda x: os.path.isfile(os.path.join(self.tensor_dir, f\"{x}.pt\"))\n",
    "            )\n",
    "            original_count = len(self.df)\n",
    "            self.df = self.df[self.df['file_exists'] == True].copy()\n",
    "            print(f\"Strict Join: Kept {len(self.df)} out of {original_count} samples.\")\n",
    "\n",
    "            if len(self.df) == 0:\n",
    "                raise FileNotFoundError(f\"No matching .pt files found in {tensor_dir}!\")\n",
    "\n",
    "        # 2. Define target labels\n",
    "        self.target_cols = ['cough', 'cold', 'asthma', 'pneumonia', 'test_status']\n",
    "\n",
    "        # 3. Preprocess labels (Standardizing 'True'/'Positive' to 1, others to 0)\n",
    "        for col in self.target_cols:\n",
    "            self.df[col] = self.df[col].apply(\n",
    "                lambda x: 1 if (x == True or (isinstance(x, str) and \n",
    "                (x.lower() in ['y', 'true', 'p', 'positive']))) else 0\n",
    "            )\n",
    "\n",
    "        # 4. Convert to Tensors for fast indexing\n",
    "        self.labels = torch.tensor(self.df[self.target_cols].values, dtype=torch.float32)\n",
    "        self.ids = self.df['id'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_id = self.ids[idx]\n",
    "        tensor_path = os.path.join(self.tensor_dir, f\"{row_id}.pt\")\n",
    "\n",
    "        try:\n",
    "            # FAST LOAD: Directly load the pre-computed spectrogram tensor\n",
    "            spectrogram = torch.load(tensor_path)\n",
    "        except Exception as e:\n",
    "            # Fallback (returns a zero tensor of the correct AST shape [1024, 128])\n",
    "            print(f\"Warning: Could not load {tensor_path} - {e}\")\n",
    "            spectrogram = torch.zeros(1024, 128)\n",
    "\n",
    "        # Returns: (Spectrogram Tensor, Label Tensor)\n",
    "        return spectrogram, self.labels[idx]\n",
    "    def get_pos_weights(self):\n",
    "        \"\"\"\n",
    "        Calculates weights directly from the internal labels tensor.\n",
    "        \"\"\"\n",
    "        # self.labels shape is [N, 5]\n",
    "        pos_counts = self.labels.sum(dim=0)  # Sum of positives for each class\n",
    "        total_samples = self.labels.size(0)\n",
    "        neg_counts = total_samples - pos_counts\n",
    "        \n",
    "        # Formula: Negatives / Positives\n",
    "        # We add a tiny epsilon to avoid division by zero\n",
    "        weights = neg_counts / (pos_counts + 1e-6)\n",
    "        \n",
    "        return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cd5998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Join: Kept 2703 out of 2746 samples.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 1. Initialize the full dataset\n",
    "# Make sure 'audio_files/' is the folder containing your .wav files\n",
    "full_dataset = OptimizedAudioDataset(csv_file='combined_data.csv', tensor_dir='processed_tensors/')\n",
    "\n",
    "# 2. Define split sizes (e.g., 80% training, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# 3. Create the DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16,       # You can likely increase batch size now\n",
    "    shuffle=True, \n",
    "    num_workers=4,       # Use more workers to load files in parallel\n",
    "    pin_memory=True      # CRITICAL: Faster CPU->GPU transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,     # No need to shuffle validation\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90a7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ASTModel\n",
    "\n",
    "class CovidAudioClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=5):\n",
    "        super(CovidAudioClassifier, self).__init__()\n",
    "        # 1. The Backbone (Body)\n",
    "        self.ast = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        # 2. The Task Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 5)  # The hidden layer allows features to \"interact\" before the final vote\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through AST\n",
    "        outputs = self.ast(x)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :] # Use the [CLS] token\n",
    "\n",
    "        # Pass through the Head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6810ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm # For a nice progress bar\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for spectrograms, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 1. Move data to GPU/CPU\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        labels = labels.to(device) # Labels are [Batch, 5]\n",
    "\n",
    "        # 2. Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(spectrograms)\n",
    "\n",
    "        # 3. Calculate Loss\n",
    "        # BCEWithLogitsLoss applies Sigmoid internally for stability\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(spectrograms)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Convert logits to binary predictions (0 or 1)\n",
    "            # We use a threshold of 0.5 after applying sigmoid\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all results for metric calculation\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Simple Accuracy: Percentage of ALL individual labels predicted correctly\n",
    "    accuracy = (all_preds == all_labels).float().mean()\n",
    "\n",
    "    return running_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d319adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 402.55it/s, Materializing param=layernorm.weight]                                 \n",
      "\u001b[1mASTModel LOAD REPORT\u001b[0m from: MIT/ast-finetuned-audioset-10-10-0.4593\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "classifier.layernorm.weight | UNEXPECTED |  | \n",
      "classifier.dense.weight     | UNEXPECTED |  | \n",
      "classifier.dense.bias       | UNEXPECTED |  | \n",
      "classifier.layernorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Robust Training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Loss: 1.6347 | Val Loss: 1.5251 | Val Acc: 0.6159\n",
      "--> NEW BEST! (Loss dropped from inf to 1.5251). Saving model.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "Train Loss: 1.3854 | Val Loss: 1.4338 | Val Acc: 0.6129\n",
      "--> NEW BEST! (Loss dropped from 1.5251 to 1.4338). Saving model.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "Train Loss: 1.2360 | Val Loss: 1.3579 | Val Acc: 0.6854\n",
      "--> NEW BEST! (Loss dropped from 1.4338 to 1.3579). Saving model.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "Train Loss: 1.0390 | Val Loss: 1.2252 | Val Acc: 0.7257\n",
      "--> NEW BEST! (Loss dropped from 1.3579 to 1.2252). Saving model.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "Train Loss: 0.9904 | Val Loss: 1.3626 | Val Acc: 0.7416\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "Train Loss: 0.8775 | Val Loss: 1.2660 | Val Acc: 0.7534\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "Train Loss: 0.7817 | Val Loss: 1.4827 | Val Acc: 0.7963\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "Train Loss: 0.6797 | Val Loss: 1.4570 | Val Acc: 0.7922\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 100%|██████████| 68/68 [00:47<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "Train Loss: 0.5762 | Val Loss: 1.3812 | Val Acc: 0.7837\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 100%|██████████| 68/68 [00:48<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "Train Loss: 0.5271 | Val Loss: 1.3293 | Val Acc: 0.8111\n",
      "--> No improvement.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     91\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 92\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# --- VALIDATION PHASE ---\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Note: We do NOT use SpecAugment here. Validation must be clean.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class MultiLabelHingeLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(MultiLabelHingeLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: Raw output from model [Batch, 5]\n",
    "        targets: 0 or 1 labels [Batch, 5]\n",
    "        \"\"\"\n",
    "        # 1. Convert targets from {0, 1} to {-1, 1}\n",
    "        # 0 -> -1, 1 -> +1\n",
    "        targets_signed = 2 * targets - 1\n",
    "        \n",
    "        # 2. Hinge Logic: max(0, 1 - y * y_pred)\n",
    "        # We want the correct class logit to be > 1.0 (margin)\n",
    "        hinge_raw = 1 - (targets_signed * logits)\n",
    "        loss = torch.clamp(hinge_raw, min=0)\n",
    "        \n",
    "        # 3. Apply Class Weights\n",
    "        # If the target is Positive (1), multiply loss by pos_weight.\n",
    "        # Otherwise, keep weight as 1.0.\n",
    "        if self.pos_weight is not None:\n",
    "            # Expand pos_weight to match batch size if necessary, though broadcasting usually handles it\n",
    "            weights = torch.where(targets == 1, self.pos_weight, torch.ones_like(logits))\n",
    "            loss = loss * weights\n",
    "            \n",
    "        return loss.mean()\n",
    "\n",
    "# 1. Device & Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 15 # Increased epochs (Augmentation makes training harder/slower, which is good)\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# 2. Model Setup (INCREASED DROPOUT)\n",
    "model = CovidAudioClassifier(num_labels=5).to(device)\n",
    "# Update the dropout layer in the existing model instance\n",
    "model.classifier[2] = nn.Dropout(p=0.5) \n",
    "\n",
    "# 3. Augmentation Transforms (SpecAugment)\n",
    "# We apply this ONLY during training\n",
    "time_masking = T.TimeMasking(time_mask_param=80) # Mask up to 80 time steps\n",
    "freq_masking = T.FrequencyMasking(freq_mask_param=40) # Mask up to 40 freq bins\n",
    "\n",
    "# 4. Loss & Optimizer\n",
    "class_weights = full_dataset.get_pos_weights().to(device)\n",
    "criterion = MultiLabelHingeLoss(pos_weight=class_weights) # Using your Hinge Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01) # Added weight_decay\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# 5. Training Loop with \"Save Best\" Logic\n",
    "print(f\"Starting Robust Training on {device}...\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # --- TRAINING PHASE ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for spectrograms, labels in train_loader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        # --- NEW: APPLY SPECAUGMENT ---\n",
    "        # AST Input shape: [Batch, Time (1024), Freq (128)]\n",
    "        # Torchaudio Masks expect: [..., Freq, Time]\n",
    "        # 1. Swap axes to [Batch, Freq, Time]\n",
    "        aug_spec = spectrograms.transpose(1, 2)\n",
    "        \n",
    "        # 2. Apply Masks\n",
    "        aug_spec = freq_masking(aug_spec)\n",
    "        aug_spec = time_masking(aug_spec)\n",
    "        \n",
    "        # 3. Swap back to [Batch, Time, Freq]\n",
    "        aug_spec = aug_spec.transpose(1, 2)\n",
    "        # -----------------------------\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(aug_spec) # Use augmented specs\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    # Note: We do NOT use SpecAugment here. Validation must be clean.\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # --- SCHEDULER & CHECKPOINTING ---\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save Best Model Logic\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"--> NEW BEST! (Loss dropped from {best_val_loss:.4f} to {val_loss:.4f}). Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        print(\"--> No improvement.\")\n",
    "        \n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47903230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved safely!\n"
     ]
    }
   ],
   "source": [
    "# Save the model currently sitting in memory\n",
    "torch.save(model.state_dict(), 'hinge_dropout.pth')\n",
    "print(\"Saved safely!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
